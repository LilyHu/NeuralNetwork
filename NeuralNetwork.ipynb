{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This is an implementation of a neural network as a class with training and prediction capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Neural network capable of both entropy loss and squared loss\n",
    "'''\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, inputLayerSize, hiddenLayerSize, outputLayerSize, loss_func_string):\n",
    "    \n",
    "        # Assign neural network loss and activiation functions\n",
    "        if loss_func_string == \"entropy\":\n",
    "            self.cost = self.entropyLoss\n",
    "            self.costPrime = self.entropyLossPrime\n",
    "        else:\n",
    "            self.cost = self.costSquaredLoss\n",
    "            self.costPrime = self.costSquaredLossPrime\n",
    "        # In the future these can be user specified\n",
    "        self.hiddenLayerActFunc = np.tanh\n",
    "        self.hiddenLayerActFuncPrime = self.tanhPrime \n",
    "        self.outputLayerActFunc = self.sigmoid\n",
    "        self.outputLayerActFuncPrime = self.sigmoidPrime\n",
    "            \n",
    "        # Layer dimensions\n",
    "        self.inputLayerSize=inputLayerSize # plus bias\n",
    "        self.hiddenLayerSize=hiddenLayerSize # plus bias\n",
    "        self.outputLayerSize=outputLayerSize\n",
    "        \n",
    "        # In the future these can be user specified\n",
    "        self.eta = None\n",
    "        np.random.seed(324)\n",
    "        self.w1=0.01*np.random.randn(self.inputLayerSize+1, self.hiddenLayerSize+1)\n",
    "        self.w2=0.01*np.random.randn(self.hiddenLayerSize+1, self.outputLayerSize)\n",
    "        \n",
    "        \n",
    "        # Document the training progress\n",
    "        self.training_accuracy = []\n",
    "        self.validation_accuracy = []\n",
    "        self.trainingTime = []\n",
    "        \n",
    "        # Current iteration variables\n",
    "        self.z1 = None\n",
    "        self.y1 = None\n",
    "        self.z2 = None\n",
    "        self.h = None # Y before one hot encoding\n",
    "        self.y = None # One hot encoding \n",
    "        self.dEdz1 = None\n",
    "        self.dEdz2 = None\n",
    "        self.Ytrain = None # Actual digits\n",
    "        self.Ytrain_encoded = None # One hot encoding\n",
    "        self.Xtrain = None # data\n",
    "\n",
    "    # Squared loss\n",
    "    def costSquaredLoss(self):\n",
    "        J=0.5*np.sum(np.square(self.y-self.h))\n",
    "        return J\n",
    "    \n",
    "    # Squared loss derivative\n",
    "    def costSquaredLossPrime(self):\n",
    "        dJdh = -(self.y-self.h)\n",
    "        return dJdh\n",
    "    \n",
    "    # Entropy loss\n",
    "    def entropyLoss(self):\n",
    "        # Need to threshold to prevent division by zero log of zero\n",
    "        h = copy.copy(self.h) # Threshold h to not modify self.h\n",
    "        h[h < 1E-5] = 1E-5\n",
    "        one_minus_h = 1-h\n",
    "        one_minus_h[one_minus_h < 1E-5] = 1E-5\n",
    "        J = - np.sum(np.multiply(self.y,np.log(h))+np.multiply((1-self.y),np.log(one_minus_h))) # * is element-wise in python as well, but np.multiply is used here to be explicit         \n",
    "        return J\n",
    "    \n",
    "    # Entropy derivative\n",
    "    def entropyLossPrime(self):\n",
    "        # Need to threshold to prevent division by zero log of zero\n",
    "        h = copy.copy(self.h) # Threshold h to not modify self.h\n",
    "        h[h < 1E-5] = 1E-5\n",
    "        one_minus_h = 1-self.h \n",
    "        one_minus_h[one_minus_h < 1E-5] = 1E-5\n",
    "        dJdh = -(self.y/h + (1-self.y)*(-1/(one_minus_h)))\n",
    "        return dJdh\n",
    "     \n",
    "    # Activication function\n",
    "    def sigmoid(self, z):\n",
    "        sig = 1/(1+np.exp(-z))\n",
    "        return sig\n",
    "    \n",
    "    # Activation function derivative\n",
    "    def sigmoidPrime(self, z):\n",
    "        sigP = np.divide(np.exp(-z),np.square(1+np.exp(-z)))\n",
    "        return sigP\n",
    "    \n",
    "    # Activication function derivative\n",
    "    def tanhPrime(self, z):\n",
    "        tanhP = np.tanh(z)\n",
    "        tanhP = 1-np.square(np.tanh(z))\n",
    "        return tanhP\n",
    "    \n",
    "    # Forward propogation\n",
    "    def propagateForward(self, X):\n",
    "        self.w1[:, -1] = 0\n",
    "        self.w1[-1,-1] = 1\n",
    "        #X = np.append(X, np.ones((len(X), 1)), axis=1) # Add a 1 for the bias\n",
    "        self.z1 = np.dot(X, self.w1)\n",
    "        self.y1 = self.hiddenLayerActFunc(self.z1)\n",
    "        #self.y1_with_bias = np.append(self.y1, np.ones((len(self.y1), 1)), axis=1) # Add a 1 for the bias\n",
    "        #self.z2 = np.dot(self.y1_with_bias, self.w2)\n",
    "        self.z2 = np.dot(self.y1, self.w2)\n",
    "        self.h = self.outputLayerActFunc(self.z2)\n",
    "        return \n",
    "    \n",
    "    # Back propagation\n",
    "    # X = Xtrain, Y = Ytrain = actual digit\n",
    "    def propagateBackward(self, X, Y):\n",
    "        # One hot encoding\n",
    "        self.y = np.zeros((len(Y), 10))\n",
    "        for i in xrange(0, len(Y)):\n",
    "            self.y[i, int(Y[i])] = 1\n",
    "        # dE/dz2 = dy_2/dz_2 * dE/dy_2\n",
    "        self.dEdz2 = np.multiply(self.outputLayerActFuncPrime(self.z2), self.costPrime())\n",
    "        # Calculate updates\n",
    "        self.dEdw2 = np.dot(self.y1.T, self.dEdz2)\n",
    "\n",
    "        # Don't back propagate the bias\n",
    "        # dE/dy_1 = dz_2/dy_1 * dEdz_2\n",
    "        self.dEdy1 = np.dot( self.dEdz2, self.w2.T)\n",
    "        self.dEdz1 = np.multiply(self.tanhPrime(self.z1), self.dEdy1)\n",
    "         # Calculate updates\n",
    "        self.dEdw1 = np.dot(X.T, self.dEdz1)\n",
    "        return \n",
    "    \n",
    "    # Train the neural network\n",
    "    def train(self, Xtrain, Ytrain, Xvalid, Yvalid, numiters, num_per_batch):\n",
    "        start_time = timeit.default_timer()\n",
    "        training_accuracy_of_last_it = 0.0\n",
    "        \n",
    "        # Initial learning rate\n",
    "        eta = 0.01\n",
    "        for it_i in xrange(0, numiters):\n",
    "            # Randomly select data\n",
    "            sample_ints = random.sample(range(0, Xtrain.shape[0]), num_per_batch)\n",
    "            X = Xtrain[sample_ints]\n",
    "            Y = Ytrain[sample_ints]\n",
    "            self.propagateForward(X)\n",
    "            self.propagateBackward(X, Y)\n",
    "            self.y = np.zeros((len(Y), 10))\n",
    "            for i in xrange(0, len(Y)):\n",
    "                self.y[i, int(Y[i])] = 1\n",
    "            \n",
    "\n",
    "            # Gradient descient updates\n",
    "            self.w1 = self.w1 - eta * self.dEdw1\n",
    "            self.w2 = self.w2 - eta * self.dEdw2\n",
    "            self.w1[:,-1] = 0\n",
    "            self.w1[-1, -1] = 1\n",
    "            \n",
    "            # Calculate accuracy rates and update the learning rate\n",
    "            if (it_i % 10000) == 0:\n",
    "                # Calculate the training time\n",
    "                end_time = timeit.default_timer()\n",
    "                self.trainingTime.append(end_time - start_time)\n",
    "                eta = 0.9*eta\n",
    "                current_training_accuracy = self.calculateAccuracyRate(Xtrain, Ytrain)\n",
    "                current_validation_accuracy = self.calculateAccuracyRate(Xvalid, Yvalid)\n",
    "                self.training_accuracy.append(current_training_accuracy)\n",
    "                self.validation_accuracy.append(current_validation_accuracy)\n",
    "                print \"Iteration number: \\t \\t\", it_i\n",
    "                print \"Time elapsed: \\t \\t \\t\", self.trainingTime[-1]\n",
    "                print \"Current training accuracy: \\t\", current_training_accuracy\n",
    "                print \"Current Validation accuracy: \\t\", current_validation_accuracy\n",
    "                training_accuracy_improvement = abs(training_accuracy_of_last_it - current_training_accuracy)\n",
    "                if (training_accuracy_improvement < 1E-7):\n",
    "                    return\n",
    "                training_accuracy_of_last_it =  current_training_accuracy \n",
    "        return\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    def calculateAccuracyRate(self, X, Y):\n",
    "        y_predicted = self.predict(X)\n",
    "        num_correct = sum(y_predicted == Y)\n",
    "        accuracy_rate = num_correct/float(Y.shape[0])\n",
    "        return accuracy_rate\n",
    "    \n",
    "    # Used for prediction\n",
    "    def predict(self, X):\n",
    "        self.propagateForward(X)\n",
    "        y_predict = np.zeros((X.shape[0]))\n",
    "        for i in xrange(0, self.h.shape[0]):\n",
    "            y_predict[i] = np.argmax(self.h[i,:])\n",
    "        return y_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics\n",
    "\n",
    "For notation, $diag(f(z))$ is a diagonal matrix with $f(x)$ on the diagonal, where $f(x)$ is a function on $x$. Also, $f'(x)$ is the derivative of the function $f(x)$\n",
    "\n",
    "$W_ij^{(1)}$ and $W_ij^{(2)}$ are  referred to as $w^l$ where $l$ is the layer.\n",
    "\n",
    "\n",
    "The stochastic gradient equation is:\n",
    "$$w^{i} = w^{i-1}-\\eta \\nabla_w J$$\n",
    "\n",
    "Where $i$ is the iteration number, $\\eta$ is the learning rate hyperparameter, and $\\nabla_w J$ is the gradient of the loss with respect to $w$. \n",
    "\n",
    "\n",
    "The elements of $\\nabla_w E$ are $\\frac{dJ}{dw_{ij}}$.\n",
    "The equation for the matrix $\\frac{dJ}{dw}$ is the following, with the superscript $l$ indicating the layer number:\n",
    "\n",
    "$$\\frac{dJ}{dw^l} = \\frac{dz^l}{dw^l} \\frac{dJ}{dz^l}$$\n",
    "\n",
    "\n",
    "For the $\\frac{dz^l}{dw^l}$ term, observe that \n",
    "$$z_j = \\sum_i {w_{ij} y_i}$$. Taking the derivative of the previous expression, we have that $$\\frac{dz^l}{dw^l} = Y^T$$\n",
    "\n",
    "\n",
    "For the term $\\frac{dJ}{dz^l}$, we have\n",
    "\n",
    "$$\\frac{dJ}{dz^l}= \\frac{dy^l}{dz^l} \\frac{dJ}{dy^l}$$\n",
    "\n",
    "For the term $\\frac{dy^l}{dz^l}$, observe that $y = f(z) = sigmoid(z)$ and $tanh(z)$ for the last layer and for inner layers respectively. Therefore,  $\\frac{dy^l}{dz^l} = sigmoid'(z)$ and $tanh'(z)$ respectively. Note that element-wise in the matrix $\\frac{dy^l}{dz^l}$, $$sigmoid'(z) = \\frac{e^{-z}}{1+e^{-z}}$$ and $$tanh'(z) = 1-tanh^2 (z)$$\n",
    "\n",
    "To take advantage of matrix calculations,  $\\frac{dy^l}{dz^l}$ can be a diagonal matrix with the derivative of each $z$ on the diagonal. (Some coding languages allow element-wise multiplication, in which case $\\frac{dy^l}{dz^l}$ can remain a vector with multiplication with other matrices done element-wise)\n",
    "\n",
    "\n",
    "\n",
    "For the output layer, $\\frac{dJ}{dy^l}$ is the derivative of the lost function. \n",
    "\n",
    "\n",
    "For squared loss\n",
    "\n",
    "$$\\frac{dJ}{dy} = \\frac{d}{dy} 1/2 (y-h)^2 =-(y-h)$$\n",
    "\n",
    "For entropy\n",
    "\n",
    "$$\\frac{dJ}{dy} = \\frac{d}{dy} -[y \\ln h + (1-y) \\ln (1-h)]= \\frac{-y}{h}+\\frac{1-y}{1-h}$$\n",
    "\n",
    "Therefore, for the $w$ between the output layer and last hidden layer, we have\n",
    "\n",
    " $$\\frac{Ey^l}{dw^l} = \\frac{dz^l}{dw^l} \\frac{dJ}{dz^l} = \\frac{dz^l}{dw^l} \\frac{dy^l}{dz^l} \\frac{dJ}{dy^l}= -Y^T diag(sigmoid'(z))\\frac{dJ}{dy^l} $$\n",
    "\n",
    "where $\\frac{dJ}{dy^l}$ is either the derivative of the squared loss or the derivative of the entropy loss. \n",
    "\n",
    "For layers below the output layer, because we have already calculated $\\frac{dJ}{dz^l}$ for the last layer, we can recursively write the following\n",
    "$$\\frac{dJ}{dz^{l-1}} = \\frac{dy^{l-1}}{dz^{l-1}} \\frac{dJ}{dy^{l-1}} =\\frac{dy^{l-1}}{dz^{l-1}} \\frac{dz^{l}}{dy^{l-1}}\\frac{dJ}{dz^{l}} $$\n",
    "\n",
    "Where $$\\frac{dz^{l}}{dy^{l-1}} = w_{l}$$ because $z_j = \\sum_i w_{ij} y_i$\n",
    "\n",
    "Putting this all together, for the $w$ between the hidden and input layers, and between hidden layers, we have\n",
    "\n",
    "$$\\frac{Ey^{l-1}}{dw^{l-1}} = \\frac{dz^{l-1}}{dw^{l-1}} \\frac{dJ}{dz^{l-1}} = \\frac{dz^{l-1}}{dw^{l-1}}\\frac{dy^{l-1}}{dz^{l-1}} \\frac{dJ}{dy^{l-1}} =\\frac{dz^{l-1}}{dw^{l-1}}\\frac{dy^{l-1}}{dz^{l-1}} \\frac{dz^{l}}{dy^{l-1}}\\frac{dJ}{dz^{l}}$$\n",
    "\n",
    "\n",
    "$$ = Y^{l-1} diag(tanh(z^{l-1})) w^l diag(sigmoid'(z)) \\frac{dJ}{dy^l} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
